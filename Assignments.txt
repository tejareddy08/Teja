-- Q) Write an SQL Query to find out call duration (in minute) for every call.

-- ğ“ğšğ›ğ¥ğ - ğŸ
CREATE TABLE call_start(
ph_no varchar(10),
start_time DATETIME);

-- ğˆğ§ğ¬ğğ«ğ­ ğ­ğ¡ğ ğğšğ­ğš 
INSERT INTO call_start VALUES
('contact_1','2024-05-01 10:20:00'),
('contact_1','2024-05-01 16:25:00'),
('contact_2','2024-05-01 12:30:00'),
('contact_3','2024-05-02 10:00:00'),
('contact_3','2024-05-02 12:30:00'),
('contact_3','2024-05-03 09:20:00');

-- ğ“ğšğ›ğ¥ğ - ğŸ
 CREATE TABLE call_end(
ph_no VARCHAR(10),
end_time DATETIME);

 -- ğˆğ§ğ¬ğğ«ğ­ ğ­ğ¡ğ ğğšğ­ğš
 INSERT INTO call_end VALUES
('contact_1','2024-05-01 10:45:00'),
('contact_1','2024-05-01 17:05:00'),
('contact_2','2024-05-01 12:55:00'),
('contact_3','2024-05-02 10:20:00'),
('contact_3','2024-05-02 12:50:00'),
('contact_3','2024-05-03 09:40:00');

--

--create pyspark dataframe 
and do operations on filter , groupby , order

 assignment 

from pyspark.sql import SparkSession 
spark =  SparkSession.builder.appName('Teja').getOrCreate()
from pyspark.sql.types import StructType,StructField,IntegerType,StringType

data = [(1,'Rahul'),(2, 'teja')]
schema = StructType([StructField("rank",IntegerType(),True),
                     StructField("Name", StringType(),True)])
df=spark.createDataFrame(data, schema)


df.display()

from pyspark.sql.functions import col
#select * from table 
df1 = df.select('*')
#select col1 from table
df1 =  df.select(col('rank'), col('name'))
df1.display()

filter 
df = df.filter(col("rank") == 1)





--drop, delete, truncate differences 
--pyspak architecture






